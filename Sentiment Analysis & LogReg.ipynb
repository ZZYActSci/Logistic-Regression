{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Logistic Regression from scratch and apply it to Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://medium.com/swlh/sentiment-analysis-from-scratch-with-logistic-regression-ca6f119256ab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-27T18:37:54.072206Z",
     "start_time": "2022-02-27T18:37:51.510168Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import twitter_samples\n",
    "positive_tweets =twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets =twitter_samples.strings('negative_tweets.json')\n",
    "example_postive_tweet=positive_tweets[0]\n",
    "example_negative_tweet=negative_tweets[0]\n",
    "test_pos = positive_tweets[4000:]\n",
    "train_pos = positive_tweets[:4000]\n",
    "test_neg = negative_tweets[4000:]\n",
    "train_neg = negative_tweets[:4000]\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  \n",
    "import string\n",
    "from nltk.corpus import stopwords          \n",
    "from nltk.stem import PorterStemmer        \n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(tweet):\n",
    "    tweet = re.sub(r'^RT[\\s]+','',tweet) # remove retweet signs\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]','',tweet) # remove https\n",
    "    tweet = re.sub(r'#','',tweet) # remove #s\n",
    "    tweet = re.sub(r'@\\w*\\s*','',tweet) # remove all names being ated.\n",
    "    tweet = re.sub(r'[0-9]*','',tweet) # remove all numbers \n",
    "    tockenizer = TweetTokenizer()\n",
    "    tweet_tokenized = tockenizer.tokenize(tweet)\n",
    "    stopwords_english = [word for word in stopwords.words('english') if word not in ['i','we','you','no']] \n",
    "    # remove stopwords but keep i, we and you (first and second pronouns), and keep 'no'\n",
    "    tweet_processed = [word for word in tweet_tokenized \n",
    "                       if word not in stopwords_english and \n",
    "                       word not in string.punctuation[1:]] # remove punctuations except !\n",
    "    stemmer = PorterStemmer()\n",
    "    tweet_after_stem = []\n",
    "    for word in tweet_processed:\n",
    "        word = stemmer.stem(word)\n",
    "        tweet_after_stem.append(word)\n",
    "    return tweet_after_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a dictionary containing the frequency of the words in the positive tweets and the second dictionary will contain the frequency of the words in the negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = []\n",
    "for tweet in positive_tweets:\n",
    "    tweet = text_process(tweet)\n",
    "    for word in tweet:\n",
    "        pos_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_pos={}\n",
    "for word in pos_words:\n",
    "    if (word,1) not in freq_pos:\n",
    "        freq_pos[(word,1)]=1\n",
    "    else:\n",
    "        freq_pos[(word,1)]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((':)', 1), 3691)\n",
      "(('!', 1), 1844)\n",
      "(('you', 1), 1464)\n",
      "(('i', 1), 1093)\n",
      "((':-)', 1), 701)\n",
      "((':d', 1), 659)\n",
      "(('thank', 1), 643)\n",
      "(('follow', 1), 447)\n",
      "(('love', 1), 400)\n",
      "(('...', 1), 290)\n",
      "(('day', 1), 246)\n",
      "(('u', 1), 245)\n",
      "(('good', 1), 238)\n",
      "(('like', 1), 232)\n",
      "(('we', 1), 228)\n",
      "(('happi', 1), 212)\n",
      "(('get', 1), 209)\n",
      "(('see', 1), 186)\n",
      "((\"i'm\", 1), 183)\n",
      "(('hi', 1), 176)\n",
      "(('great', 1), 172)\n"
     ]
    }
   ],
   "source": [
    "items = dict(sorted(freq_pos.items(), key=lambda item: item[1],reverse=True)).items()\n",
    "n=0\n",
    "for i in items:\n",
    "    print(i)\n",
    "    n+=1\n",
    "    if n>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = []\n",
    "for tweet in negative_tweets:\n",
    "    tweet = text_process(tweet)\n",
    "    for word in tweet:\n",
    "        neg_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_neg = {}\n",
    "for word in neg_words:\n",
    "    if (word,0) not in freq_neg:\n",
    "        freq_neg[(word,0)]=1\n",
    "    else:\n",
    "        freq_neg[(word,0)]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((':(', 0), 4585)\n",
      "(('i', 0), 2207)\n",
      "(('!', 0), 829)\n",
      "(('you', 0), 744)\n",
      "((':-(', 0), 501)\n",
      "((\"i'm\", 0), 343)\n",
      "(('...', 0), 332)\n",
      "(('miss', 0), 301)\n",
      "(('no', 0), 285)\n",
      "(('pleas', 0), 275)\n",
      "(('follow', 0), 263)\n",
      "(('want', 0), 246)\n",
      "(('get', 0), 233)\n",
      "(('go', 0), 223)\n",
      "(('like', 0), 223)\n",
      "(('♛', 0), 210)\n",
      "(('》', 0), 210)\n",
      "(('u', 0), 193)\n",
      "((\"can't\", 0), 180)\n",
      "(('me', 0), 174)\n",
      "(('time', 0), 166)\n"
     ]
    }
   ],
   "source": [
    "items = dict(sorted(freq_neg.items(), key=lambda item: item[1],reverse=True)).items()\n",
    "n=0\n",
    "for i in items:\n",
    "    print(i)\n",
    "    n+=1\n",
    "    if n>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def feature_extraction(tweet,freq_pos,freq_neg):\n",
    "    word_l=text_process(tweet)\n",
    "    x = np.zeros((1,7))\n",
    "    x[0,0] = 1\n",
    "    for word in word_l:\n",
    "        try:\n",
    "            x[0,1]+=freq_pos[(word,1)]\n",
    "        except:\n",
    "            x[0,1]+=0\n",
    "        try:\n",
    "            x[0,2]+=freq_neg[(word,0)]\n",
    "        except:\n",
    "            x[0,2]+=0\n",
    "        if word in ['i','we','u','you']:\n",
    "            x[0,4] += 1\n",
    "\n",
    "    if 'no' in word_l:\n",
    "        x[0,3] = 1\n",
    "    if '!' in word_l:\n",
    "        x[0,5] = 1\n",
    "    x[0,6] = np.log(len(word_l))\n",
    "    \n",
    "    assert x.shape == (1,7),'Shape is not correct'\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(train_x),7))\n",
    "for i in range(len(train_x)):\n",
    "    X_train[i,:] = feature_extraction(train_x[i],freq_pos,freq_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 3.88900000e+03 7.40000000e+01 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.94591015e+00]\n",
      " [1.00000000e+00 1.00160000e+04 3.10600000e+03 ... 2.00000000e+00\n",
      "  1.00000000e+00 2.89037176e+00]\n",
      " [1.00000000e+00 8.90200000e+03 2.68000000e+03 ... 3.00000000e+00\n",
      "  1.00000000e+00 2.63905733e+00]\n",
      " ...\n",
      " [1.00000000e+00 2.37000000e+03 5.41300000e+03 ... 2.00000000e+00\n",
      "  0.00000000e+00 2.19722458e+00]\n",
      " [1.00000000e+00 1.35100000e+03 7.07600000e+03 ... 1.00000000e+00\n",
      "  0.00000000e+00 2.07944154e+00]\n",
      " [1.00000000e+00 1.33500000e+03 7.18800000e+03 ... 1.00000000e+00\n",
      "  0.00000000e+00 2.39789527e+00]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the model mathematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 6 features in the model  \n",
    "feature 1: sum of count of words that occur in positive tweets  \n",
    "feature 2: sum of count of words that occur in negative tweets  \n",
    "feature 3: binary; if 'no' is in tweet  \n",
    "feature 4: sum of count of 1st and 2nd pronouns in the tweet  \n",
    "feature 5: binary; if '!' is in tweet  \n",
    "feature 6: log of number of words in tweet  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ log(\\frac{p}{1-p}) = \\beta_0 + \\beta_{1}*\\#positivity + \\beta_{2}*\\#negativity + \\beta_{3} * no\\_in\\_doc + \\beta_{4} * 1st\\_2nd\\_pronoun +  \\beta_{5} * !\\_in\\_doc + \\beta_{6} * log(\\#words)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Write the likelihood function for your logistic regression model obtained in (i)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\prod^{n}_{i=1} p(x_i)^{y_i} * (1-p(x_i))^{1-y_i} $$\n",
    "where $$p(x_i) = \\frac{1}{1+e^{-\\sum(\\beta_{j}*x_{ij})}}$$ where $$x_{i0} = 1$$ and $$j \\in \\{0,1,2,3,4,5,6\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the logistic regression classifier using a black-box implementation and evaluate its performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='none')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "black_box = LogisticRegression(penalty='none')\n",
    "black_box.fit(X_train,train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.zeros((len(test_x),7))\n",
    "for i in range(len(test_x)):\n",
    "    X_test[i,:] = feature_extraction(test_x[i],freq_pos,freq_neg)\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = black_box.predict(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9755"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_y.ravel(),y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45121617]\n",
      "[ 6.57621086 -6.86634029 -0.16300439  0.76632607 -1.15679155 -0.11777955]\n"
     ]
    }
   ],
   "source": [
    "print(black_box.intercept_)\n",
    "print(black_box.coef_[0][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the logistic regression classifier by minimizing the negative log-likelihood function using a numerical optimization procedure: gradient descent or stochastic gradient descent. Compare with the coefficients obtained in step (iii)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Xbeta):\n",
    "    return 1/(1+np.exp(-Xbeta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.30545102, -1.11510595, ..., -0.70881191,\n",
       "        -0.46713197,  0.04191112],\n",
       "       [ 1.        ,  2.46057506, -0.1536601 , ...,  1.52059454,\n",
       "         2.14072269,  1.5865076 ],\n",
       "       [ 1.        ,  2.06873433, -0.28874451, ...,  2.63529777,\n",
       "         2.14072269,  1.1755016 ],\n",
       "       ...,\n",
       "       [ 1.        , -0.22884528,  0.57788857, ...,  1.52059454,\n",
       "        -0.46713197,  0.45291711],\n",
       "       [ 1.        , -0.58727051,  1.10522513, ...,  0.40589131,\n",
       "        -0.46713197,  0.26029175],\n",
       "       [ 1.        , -0.59289838,  1.14074028, ...,  0.40589131,\n",
       "        -0.46713197,  0.78109906]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,0] = np.ones(X_train.shape[0])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.0000001 # learning rate\n",
    "beta = np.zeros((7,1)) # initialize the 7 coefficients\n",
    "iters = 1000000000\n",
    "negLog_L_old=0\n",
    "precision = 0.001\n",
    "for k in range(iters):\n",
    "    negLog_L =  - (train_y.T @ np.log(sigmoid(X_train @ beta)) + (1-train_y.T) @ np.log(1-sigmoid(X_train @ beta)))\n",
    "    beta = beta - gamma * -1 * X_train.T @ (train_y-sigmoid(X_train @ beta))\n",
    "    if np.abs(negLog_L_old-negLog_L) < precision:\n",
    "        break\n",
    "    else:\n",
    "        negLog_L_old = negLog_L \n",
    "    #print(negLog_L_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14933244]\n",
      " [ 2.83777927]\n",
      " [-3.4955372 ]\n",
      " [-0.15035754]\n",
      " [ 0.39991644]\n",
      " [-0.25972685]\n",
      " [-0.02214746]]\n"
     ]
    }
   ],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent with exponentialy decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.0001 # exponentialy decaying rate\n",
    "beta = np.zeros((7,1)) # initialize the 7 coefficients\n",
    "iters = 1000000000\n",
    "negLog_L_old=0\n",
    "precision = 0.00001\n",
    "for k in range(iters):\n",
    "    negLog_L =  - (train_y.T @ np.log(sigmoid(X_train @ beta)) + (1-train_y.T) @ np.log(1.0001-sigmoid(X_train @ beta)))\n",
    "    if pd.isna(negLog_L[0][0]):\n",
    "        break\n",
    "    beta = beta - gamma * np.exp(-0.00001*(k+1)) * -1 * X_train.T @ (train_y-sigmoid(X_train @ beta))\n",
    "    if np.abs(negLog_L_old-negLog_L) < precision:\n",
    "        break\n",
    "    else:\n",
    "        negLog_L_old = negLog_L \n",
    "    #print(negLog_L_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.45013639],\n",
       "       [ 6.56400638],\n",
       "       [-6.85476236],\n",
       "       [-0.16305368],\n",
       "       [ 0.7649779 ],\n",
       "       [-1.15408355],\n",
       "       [-0.11770085]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_result = beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01 # exponentialy decaying learning rate\n",
    "beta = np.zeros((7,1)) # initialize the 7 coefficients\n",
    "iters = 1000000000\n",
    "negLog_L_old=0\n",
    "precision = 0.0001\n",
    "for k in range(iters):\n",
    "    negLog_L =  - (train_y.T @ np.log(sigmoid(X_train @ beta)) + (1-train_y.T) @ np.log(1-sigmoid(X_train @ beta)))\n",
    "    if pd.isna(negLog_L[0][0]):\n",
    "        break\n",
    "    #beta = beta - gamma * np.exp(-0.00001*(k+1)) * -1 * X_train.T @ (train_y-sigmoid(X_train @ beta))\n",
    "    idx = np.random.randint(8000)\n",
    "    beta = beta - gamma * np.exp(-0.0001*(k+1)) * -1 * X_train.T[:,idx].reshape((7,1)) @ (train_y[idx] - sigmoid(X_train[idx,:]@beta)).reshape((1,1))\n",
    "    if np.abs(negLog_L_old-negLog_L) < precision:\n",
    "        break\n",
    "    else:\n",
    "        negLog_L_old = negLog_L \n",
    "    #print(negLog_L_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.05639091]\n",
      " [ 1.59062501]\n",
      " [-2.14329671]\n",
      " [-0.09604767]\n",
      " [ 0.1417191 ]\n",
      " [-0.01338122]\n",
      " [ 0.10488665]]\n"
     ]
    }
   ],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.01 # exponentialy decaying learning rate\n",
    "beta = np.zeros((7,1)) # initialize the 7 coefficients\n",
    "iters = 1000000000\n",
    "negLog_L_old=0\n",
    "precision = 0.0001\n",
    "for k in range(iters):\n",
    "    negLog_L =  - (train_y.T @ np.log(sigmoid(X_train @ beta)) + (1-train_y.T) @ np.log(1.00001-sigmoid(X_train @ beta)))\n",
    "    if pd.isna(negLog_L[0][0]):\n",
    "        break\n",
    "    #beta = beta - gamma * np.exp(-0.00001*(k+1)) * -1 * X_train.T @ (train_y-sigmoid(X_train @ beta))\n",
    "    idx = np.random.randint(8000,size=30)\n",
    "    beta = beta - gamma * np.exp(-0.0001*(k+1)) * -1 * X_train.T[:,idx] @ (train_y[idx] - sigmoid(X_train[idx,:]@beta))\n",
    "    if np.abs(negLog_L_old-negLog_L) < precision:\n",
    "        break\n",
    "    else:\n",
    "        negLog_L_old = negLog_L \n",
    "    #print(negLog_L_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.33402575]\n",
      " [ 4.64684221]\n",
      " [-5.34238567]\n",
      " [-0.1872796 ]\n",
      " [ 0.60112884]\n",
      " [-0.87487977]\n",
      " [-0.08791036]]\n"
     ]
    }
   ],
   "source": [
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Gradient Descent Result to predict for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)\n",
    "X_test[:,0] = np.ones(X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sigmoid(X_test@beta_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_category = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        y_category.append(1)\n",
    "    else:\n",
    "        y_category.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = 0\n",
    "for i in range(2000):\n",
    "    if y_category[i]==test_y[i]:\n",
    "        match +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9755\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: ', match/2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model achieves similar accuracy as the black box implementation (i.e., Logistic Regression with no penalty). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45121617]\n",
      "[ 6.57621086 -6.86634029 -0.16300439  0.76632607 -1.15679155 -0.11777955]\n"
     ]
    }
   ],
   "source": [
    "print(black_box.intercept_)\n",
    "print(black_box.coef_[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.45013639]\n",
      " [ 6.56400638]\n",
      " [-6.85476236]\n",
      " [-0.16305368]\n",
      " [ 0.7649779 ]\n",
      " [-1.15408355]\n",
      " [-0.11770085]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(beta_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We used the default solver of LogisticRegression, which is lbfgs, and set penalty = None.  Before feeding into the black box, we scale the data to help the result converge and more stable. The black box result puts a higher weight on the first two features, which are the counts of positive and negative words. The remaining features have a relatively small effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For our manual computation, we can see the prediction is as excellent as the black box version with an accuracy of 0.9755. Furthermore, the coefficients we obtained manually are roughly the same as the black box's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
